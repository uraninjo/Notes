{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAGGLE FEATURE ENGINEERING NOTLARI\n",
    "\n",
    "İlk derste aralarındaki ilişkiyi daha net göstermek için bir metod olan orantısallığı gördüm. Bunu derken kastettiğim, x ve y maddesinin arasındaki fark makine için açık olmayabilir. Bunun için bizim yapmamız gerek iki kolon yerine bir kolona onların arasında oranı koymak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İkinci derste mutual information olarak adlandırılan bir teknik gördüm. Bu teknik sklearnde bulunan bir teknik.\n",
    "**sklearn.feature_selection.mutual_info_classif**\n",
    "\n",
    "Bununla beraber aslında hedefimizdeki data ile aralarındaki bağlantıyı daha kolay görebiliriz. Bir çeşit koralasyon gibi düşünülebilir.\n",
    "Araştırırken bulduğum bir kaynak var. Buradan rahatça ne olduğu anlaşılabilir.\n",
    "\n",
    "**https://towardsdatascience.comselect-features-for-machine-learning-model-with-mutual-information-534fe387d5c8**\n",
    "\n",
    "Mutual information olarak adlandırılan metrik 0 olduğunda hedef ve elimizdeki data arasında bir ölçülebilir bir bağlantı olmadığını gösteriyor. 2.0'ın üstündeki değerler genelde sık karşılaşılmayan değerlerdir...\n",
    "\n",
    "\n",
    "![](2022-05-03-14-27-19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Üst tarafta mutual information metriği için bir skor ölçme fonksiyonu yazıyor. Altta ise bunları görselleştirmek için bir fonksiyon var..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n",
    "\n",
    "\n",
    "plt.figure(dpi=100, figsize=(8, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alttaki kodlar bu işler için iyi bir kod gösterimi olduğu için koyuyorum..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"YearBuilt\", \"MoSold\", \"ScreenPorch\"]\n",
    "sns.relplot(\n",
    "    x=\"value\", y=\"SalePrice\", col=\"variable\", data=df.melt(id_vars=\"SalePrice\", value_vars=features), facet_kws=dict(sharex=False),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"GrLivArea\"\n",
    "\n",
    "sns.lmplot(\n",
    "    x=feature, y=\"SalePrice\", hue=\"BldgType\", col=\"BldgType\",\n",
    "    data=df, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir veriyi normalize ederken kullanacağımız teknikler önemlidir. Mesela alttaki fotoğrafta, veri ile veriyi np.log1p ile normalize edilmiş hallerini görüyorsunuz...\n",
    "\n",
    "Bu bize normal dağılıma daha yakın bir görüntü sunabilir...\n",
    "\n",
    "\n",
    "![](2022-05-03-14-59-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bazen yapacağınız işleme göre değişkenlik gösterebilecek şekilde de olsa verileri 'count' kullanarak da yapabiliriz. Mesela elimizdeki verilere bakarak olup olmamasına göre True veya False yaparak olduklarındaki değişimi bir dataframede saklayabiliriz. Bu işlerimizi kolaylaştırabilir...\n",
    "\n",
    "Aynı şekilde işlerimizi kolaylaştırması açısından gruplayarak da datamızı şekillendirebiliriz. Aşağıda bunun için örnekler vardır..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer[\"AverageIncome\"] = (\n",
    "    customer.groupby(\"State\")  # for each state\n",
    "    [\"Income\"]                 # select the income\n",
    "    .transform(\"mean\")         # and compute its mean\n",
    ")\n",
    "#********************\n",
    "customer[\"StateFreq\"] = (\n",
    "    customer.groupby(\"State\")\n",
    "    [\"State\"]\n",
    "    .transform(\"count\")\n",
    "    / customer.State.count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips on Creating Features\n",
    "It's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:\n",
    "* Linear models learn sums and differences naturally, but can't learn anything more complex.\n",
    "* Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n",
    "* Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n",
    "* Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\n",
    "* Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLUSTERING WITH K-MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basitçe açıklamak gerekirse clustering(kümeleme) olarak bahsettiğimiz, benzer özelliklere sahip verilerin gruplandırılması,kümelendirilmesidir.\n",
    "\n",
    "Aşağıdaki örnekler tek özellik ve iki özelliğe sahip clustering işlemi uygulanmış  grafiklerdir...\n",
    "\n",
    "![](2022-05-05-14-41-59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn'de clustering için bulunan 3 parametre var. \n",
    "\n",
    "**n_clusters, max_iter, n_init**\n",
    "\n",
    "n_clusters parametresi kadar rastgele clusterla başlayan bu işlem, diğer parametrelerin optimize bir şekilde olmasıyla uzaklıkların minumum olabilecek şekilde clusterlar belirlenir. Çoğu zaman, ağırlık merkezlerinin başlangıçtaki rastgele konumu, zayıf bir kümeleme ile sonuçlanır. Bu yüzden n_init'e iyi karar verilmelidir. Ayrıca yineleme sayısı yani max_iter'in doğru noktanın bulunması için önemlidir.\n",
    "\n",
    "\n",
    "![](1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster feature\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "X[\"Cluster\"] = kmeans.fit_predict(X)\n",
    "X[\"Cluster\"] = X[\"Cluster\"].astype(\"category\")\n",
    "\n",
    "\n",
    "sns.relplot(\n",
    "    x=\"Longitude\", y=\"Latitude\", hue=\"Cluster\", data=X, height=6,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kümelemenin yakınlığa dayalı olarak veri kümesinin bölümlenmesi olduğu gibi, PCA'yı da verilerdeki varyasyonun bölümlenmesi olarak düşünebilirsiniz. PCA, verilerdeki önemli ilişkileri keşfetmenize yardımcı olacak harika bir araçtır ve daha bilgilendirici özellikler oluşturmak için de kullanılabilir.\n",
    "\n",
    "(Teknik not: PCA tipik olarak standartlaştırılmış verilere uygulanır. Standartlaştırılmış verilerle \"varyasyon\", \"korelasyon\" anlamına gelir. Standartlaştırılmamış verilerle \"varyasyon\", \"kovaryans\" anlamına gelir. Bu kurstaki tüm veriler, PCA uygulanmadan önce standartlaştırılacaktır.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that instead of describing abalones by their 'Height' and 'Diameter', we could just as well describe them by their 'Size' and 'Shape'. This, in fact, is the whole idea of PCA: instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways you could use PCA for feature engineering.\n",
    "\n",
    "The first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create -- a product of 'Height' and 'Diameter' if 'Size' is important, say, or a ratio of 'Height' and 'Diameter' if Shape is important. You could even try clustering on one or more of the high-scoring components.\n",
    "\n",
    "The second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\n",
    "\n",
    "* Dimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\n",
    "* Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\n",
    "* Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\n",
    "* Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n",
    "PCA basically gives you direct access to the correlational structure of your data. You'll no doubt come up with applications of your own!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
